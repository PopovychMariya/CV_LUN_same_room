{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01efc7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dda67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ba3a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df4f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a0d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabs intentional (your preference)\n",
    "import os, csv, math, json, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, f1_score\n",
    "\n",
    "# -------------------\n",
    "# Paths\n",
    "# -------------------\n",
    "BASE_DIR = Path(\"/content/drive/MyDrive\")  # <- change if needed\n",
    "RESEARCH = BASE_DIR / \"research\"\n",
    "GRID_DIR = RESEARCH / \"keypoints\" / \"train\"\n",
    "LABEL_CSV = RESEARCH / \"dataset\" / \"train_labels.csv\"\n",
    "OUT_DIR = RESEARCH / \"runs\" / \"grid_cnn\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 1337\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# -------------------\n",
    "# Data utils\n",
    "# -------------------\n",
    "def read_labels(csv_fp):\n",
    "\tlabels = {}\n",
    "\twith open(csv_fp, newline=\"\", encoding=\"utf-8\") as f:\n",
    "\t\treader = csv.DictReader(f)\n",
    "\t\tfor r in reader:\n",
    "\t\t\ttid = str(r[\"task_id\"]).strip()\n",
    "\t\t\tlb = int(r[\"label\"])\n",
    "\t\t\tlabels[tid] = lb\n",
    "\treturn labels\n",
    "\n",
    "def find_samples(grid_root, labels):\n",
    "\tsamples = []\n",
    "\tfor tid, y in labels.items():\n",
    "\t\tfp = grid_root / tid / \"grid_G32.npy\"\n",
    "\t\tif fp.exists():\n",
    "\t\t\tsamples.append((tid, fp, y))\n",
    "\treturn samples\n",
    "\n",
    "class GridDataset(Dataset):\n",
    "\tdef __init__(self, samples, mean=None, std=None, add_coord=True):\n",
    "\t\tself.samples = samples\n",
    "\t\tself.add_coord = add_coord\n",
    "\t\tself._mean = mean\n",
    "\t\tself._std = std\n",
    "\n",
    "\tdef __len__(self): return len(self.samples)\n",
    "\n",
    "\tdef set_norm(self, mean, std):\n",
    "\t\tself._mean = mean; self._std = std\n",
    "\n",
    "\tdef _load_grid(self, fp):\n",
    "\t\tarr = np.load(fp)\n",
    "\t\t# expect [C,32,32] or [32,32,C]\n",
    "\t\tif arr.ndim == 2:\n",
    "\t\t\tarr = arr[None, ...]  # [1,H,W]\n",
    "\t\tif arr.shape[0] == 32 and arr.shape[1] == 32:\n",
    "\t\t\tarr = np.transpose(arr, (2,0,1))  # -> [C,32,32]\n",
    "\t\treturn arr.astype(np.float32)\n",
    "\n",
    "\tdef __getitem__(self, i):\n",
    "\t\ttid, fp, y = self.samples[i]\n",
    "\t\tx = self._load_grid(fp)  # [C,32,32]\n",
    "\t\tC, H, W = x.shape\n",
    "\n",
    "\t\tif self.add_coord:\n",
    "\t\t\tii = np.linspace(0, 1, H, dtype=np.float32)\n",
    "\t\t\tjj = np.linspace(0, 1, W, dtype=np.float32)\n",
    "\t\t\tyy, xx = np.meshgrid(ii, jj, indexing=\"ij\")\n",
    "\t\t\tcoord = np.stack([yy, xx], axis=0)  # [2,H,W]\n",
    "\t\t\tx = np.concatenate([x, coord], axis=0)\n",
    "\t\t\tC += 2\n",
    "\n",
    "\t\tif self._mean is not None and self._std is not None:\n",
    "\t\t\t# per-channel normalize\n",
    "\t\t\tx = (x - self._mean[:C, None, None]) / (self._std[:C, None, None] + 1e-6)\n",
    "\n",
    "\t\treturn torch.from_numpy(x), torch.tensor([y], dtype=torch.float32), tid\n",
    "\n",
    "def compute_channel_stats(dataset):\n",
    "\t# pass with no normalization\n",
    "\tsum_c = None; sumsq_c = None; n_pix = 0\n",
    "\tfor k in range(len(dataset)):\n",
    "\t\tx, _, _ = dataset[k]\n",
    "\t\tx = x.numpy()\n",
    "\t\tC, H, W = x.shape\n",
    "\t\tif sum_c is None:\n",
    "\t\t\tsum_c = np.zeros(C, dtype=np.float64)\n",
    "\t\t\tsumsq_c = np.zeros(C, dtype=np.float64)\n",
    "\t\tsum_c += x.reshape(C, -1).sum(axis=1)\n",
    "\t\tsumsq_c += (x.reshape(C, -1) ** 2).sum(axis=1)\n",
    "\t\tn_pix += H * W\n",
    "\tmean = sum_c / n_pix\n",
    "\tvar = np.maximum(sumsq_c / n_pix - mean**2, 0.0)\n",
    "\tstd = np.sqrt(var)\n",
    "\treturn mean.astype(np.float32), std.astype(np.float32)\n",
    "\n",
    "# -------------------\n",
    "# Model\n",
    "# -------------------\n",
    "class ConvBlock(nn.Module):\n",
    "\tdef __init__(self, c_in, c_out, k=3, s=1, p=1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.conv = nn.Conv2d(c_in, c_out, k, s, p, bias=False)\n",
    "\t\tself.bn = nn.BatchNorm2d(c_out)\n",
    "\t\tself.act = nn.ReLU(inplace=True)\n",
    "\tdef forward(self, x): return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "class ResidBlock(nn.Module):\n",
    "\tdef __init__(self, c, widen=1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tmid = c * widen\n",
    "\t\tself.b1 = ConvBlock(c, mid)\n",
    "\t\tself.b2 = ConvBlock(mid, c)\n",
    "\tdef forward(self, x): return x + self.b2(self.b1(x))\n",
    "\n",
    "class JointGridCNN(nn.Module):\n",
    "\tdef __init__(self, in_ch, base=48, p_drop=0.2):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.stem = nn.Sequential(\n",
    "\t\t\tConvBlock(in_ch, base, k=5, s=2, p=2),   # 32->16\n",
    "\t\t\tResidBlock(base),\n",
    "\t\t\tConvBlock(base, base*2, k=3, s=2, p=1), # 16->8\n",
    "\t\t\tResidBlock(base*2),\n",
    "\t\t\tResidBlock(base*2)\n",
    "\t\t)\n",
    "\t\tself.se = nn.Sequential(\n",
    "\t\t\tnn.AdaptiveAvgPool2d(1),\n",
    "\t\t\tnn.Conv2d(base*2, base*2//4, 1), nn.ReLU(True),\n",
    "\t\t\tnn.Conv2d(base*2//4, base*2, 1), nn.Sigmoid()\n",
    "\t\t)\n",
    "\t\tself.head = nn.Sequential(\n",
    "\t\t\tnn.Dropout(p_drop),\n",
    "\t\t\tnn.Linear(base*2, 128), nn.ReLU(True),\n",
    "\t\t\tnn.Dropout(p_drop),\n",
    "\t\t\tnn.Linear(128, 1)  # logit\n",
    "\t\t)\n",
    "\tdef forward(self, x):\n",
    "\t\th = self.stem(x)\n",
    "\t\th = h * self.se(h)\n",
    "\t\th = F.adaptive_avg_pool2d(h, 1).flatten(1)\n",
    "\t\treturn self.head(h)\n",
    "\n",
    "# -------------------\n",
    "# Train / Eval\n",
    "# -------------------\n",
    "def eval_metrics(y_true, y_score, threshold=None):\n",
    "\troc = roc_auc_score(y_true, y_score)\n",
    "\tpr = average_precision_score(y_true, y_score)\n",
    "\tif threshold is None:\n",
    "\t\t# find best-F1 threshold on PR curve\n",
    "\t\tp, r, t = precision_recall_curve(y_true, y_score)\n",
    "\t\tf1s = (2 * p * r) / np.maximum(p + r, 1e-9)\n",
    "\t\tbest_idx = int(np.nanargmax(f1s))\n",
    "\t\tbest_thr = 0.5 if best_idx >= len(t) else t[best_idx]\n",
    "\telse:\n",
    "\t\tbest_thr = threshold\n",
    "\ty_pred = (y_score >= best_thr).astype(np.int32)\n",
    "\tf1 = f1_score(y_true, y_pred)\n",
    "\treturn {\"roc_auc\": roc, \"pr_auc\": pr, \"f1\": f1, \"thr\": float(best_thr)}\n",
    "\n",
    "def train_one_epoch(model, loader, opt, loss_fn, dev):\n",
    "\tmodel.train()\n",
    "\ttotal = 0.0\n",
    "\tfor x, y, _ in loader:\n",
    "\t\tx = x.to(dev, non_blocking=True)\n",
    "\t\ty = y.to(dev, non_blocking=True)\n",
    "\t\topt.zero_grad(set_to_none=True)\n",
    "\t\tlogit = model(x)\n",
    "\t\tloss = loss_fn(logit, y)\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\ttotal += float(loss.item()) * x.size(0)\n",
    "\treturn total / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, dev):\n",
    "\tmodel.eval()\n",
    "\tall_y, all_s = [], []\n",
    "\tfor x, y, _ in loader:\n",
    "\t\tx = x.to(dev, non_blocking=True)\n",
    "\t\tlogit = model(x)\n",
    "\t\tall_s.append(torch.sigmoid(logit).cpu().numpy().ravel())\n",
    "\t\tall_y.append(y.cpu().numpy().ravel())\n",
    "\treturn np.concatenate(all_y), np.concatenate(all_s)\n",
    "\n",
    "# -------------------\n",
    "# Orchestration\n",
    "# -------------------\n",
    "labels = read_labels(LABEL_CSV)\n",
    "samples = find_samples(GRID_DIR, labels)\n",
    "assert len(samples) > 0, \"No matching grid files found.\"\n",
    "\n",
    "# stratified split\n",
    "tids = [s[0] for s in samples]\n",
    "ys = [s[2] for s in samples]\n",
    "train_ids, val_ids = train_test_split(\n",
    "\tnp.arange(len(samples)), test_size=0.2, random_state=SEED, stratify=ys\n",
    ")\n",
    "\n",
    "train_samples = [samples[i] for i in train_ids]\n",
    "val_samples   = [samples[i] for i in val_ids]\n",
    "\n",
    "# Optional: split val into tune/eval if you want to pick threshold on a separate split\n",
    "USE_TWO_VALS = False\n",
    "if USE_TWO_VALS:\n",
    "\tval_tune_idx, val_hold_idx = train_test_split(\n",
    "\t\tnp.arange(len(val_samples)), test_size=0.5, random_state=SEED, stratify=[s[2] for s in val_samples]\n",
    "\t)\n",
    "\tval_tune = [val_samples[i] for i in val_tune_idx]\n",
    "\tval_hold = [val_samples[i] for i in val_hold_idx]\n",
    "else:\n",
    "\tval_tune, val_hold = val_samples, None\n",
    "\n",
    "# build datasets\n",
    "ds_train = GridDataset(train_samples, mean=None, std=None, add_coord=True)\n",
    "mean, std = compute_channel_stats(ds_train)\n",
    "ds_train.set_norm(mean, std)\n",
    "ds_val_tune = GridDataset(val_tune, mean=mean, std=std, add_coord=True)\n",
    "dl_train = DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "dl_val_tune = DataLoader(ds_val_tune, batch_size=256, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "if val_hold is not None:\n",
    "\tds_val_hold = GridDataset(val_hold, mean=mean, std=std, add_coord=True)\n",
    "\tdl_val_hold = DataLoader(ds_val_hold, batch_size=256, shuffle=False, num_workers=2, pin_memory=True, persistent_workers=True)\n",
    "\n",
    "# model\n",
    "# infer input channels from one sample\n",
    "x0, _, _ = ds_train[0]\n",
    "in_ch = x0.shape[0]\n",
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = JointGridCNN(in_ch=in_ch, base=48, p_drop=0.2).to(dev)\n",
    "\n",
    "# class imbalance handling\n",
    "pos_ratio = float(np.mean([s[2] for s in train_samples]))\n",
    "pos_weight = torch.tensor([(1.0 - pos_ratio) / max(pos_ratio, 1e-6)], device=dev)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "\n",
    "best_pr = -1.0\n",
    "best_state = None\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "\ttrain_loss = train_one_epoch(model, dl_train, opt, loss_fn, dev)\n",
    "\tyv, sv = predict(model, dl_val_tune, dev)\n",
    "\tmetrics = eval_metrics(yv, sv)  # tunes threshold on val_tune\n",
    "\tsched.step()\n",
    "\n",
    "\tprint(f\"epoch {epoch:02d} | loss {train_loss:.4f} | roc {metrics['roc_auc']:.4f} | pr {metrics['pr_auc']:.4f} | f1 {metrics['f1']:.4f} | thr {metrics['thr']:.3f}\")\n",
    "\n",
    "\tif metrics[\"pr_auc\"] > best_pr:\n",
    "\t\tbest_pr = metrics[\"pr_auc\"]\n",
    "\t\tbest_state = {\n",
    "\t\t\t\"epoch\": epoch,\n",
    "\t\t\t\"model\": model.state_dict(),\n",
    "\t\t\t\"mean\": mean, \"std\": std,\n",
    "\t\t\t\"threshold\": metrics[\"thr\"],\n",
    "\t\t\t\"in_ch\": in_ch\n",
    "\t\t}\n",
    "\n",
    "# final eval\n",
    "if val_hold is not None:\n",
    "\tmodel.load_state_dict(best_state[\"model\"])\n",
    "\tyh, sh = predict(model, dl_val_hold, dev)\n",
    "\tfinal_m = eval_metrics(yh, sh, threshold=best_state[\"threshold\"])\n",
    "\tprint(f\"[HOLDOUT] roc {final_m['roc_auc']:.4f} | pr {final_m['pr_auc']:.4f} | f1 {final_m['f1']:.4f} | thr {final_m['thr']:.3f}\")\n",
    "\n",
    "# save checkpoint\n",
    "ckpt_fp = OUT_DIR / \"jointgridcnn_best.pth\"\n",
    "torch.save(best_state, ckpt_fp)\n",
    "with open(OUT_DIR / \"meta.json\", \"w\") as f:\n",
    "\tjson.dump({\n",
    "\t\t\"best_epoch\": best_state[\"epoch\"],\n",
    "\t\t\"threshold\": best_state[\"threshold\"],\n",
    "\t\t\"train_size\": len(train_samples),\n",
    "\t\t\"val_size\": len(val_samples),\n",
    "\t\t\"use_two_vals\": USE_TWO_VALS\n",
    "\t}, f, indent=2)\n",
    "\n",
    "print(f\"Saved: {ckpt_fp}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
