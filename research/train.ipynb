{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40293603",
      "metadata": {
        "id": "40293603"
      },
      "source": [
        "## COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2a1a0d4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a1a0d4c",
        "outputId": "fbe17057-01b9-42d1-cc4d-2acb86e6f467"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "efc46d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efc46d38",
        "outputId": "ef231590-8ba4-4d00-b618-c0902db3026b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/drive/MyDrive/CV_LUN_same_room'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 65 (delta 15), reused 63 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (65/65), 63.02 KiB | 5.73 MiB/s, done.\n",
            "Resolving deltas: 100% (15/15), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/PopovychMariya/CV_LUN_same_room /content/drive/MyDrive/CV_LUN_same_room"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "D6tz5n03HolD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6tz5n03HolD",
        "outputId": "457bfde7-690b-477b-f442-d6dc318fd70a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/CV_LUN_same_room_research\n"
          ]
        }
      ],
      "source": [
        "!cp -r /content/drive/MyDrive/CV_LUN_same_room/research /content/CV_LUN_same_room_research\n",
        "%cd /content/CV_LUN_same_room_research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4dz-Y-tesmyg",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dz-Y-tesmyg",
        "outputId": "f0e7b259-b70e-4a71-9e5c-c7f170689cb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.4.0 (from -r requirements.txt (line 1))\n",
            "  Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting torchvision==0.19.0 (from -r requirements.txt (line 2))\n",
            "  Downloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: timm>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.0.21)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.8.1.78 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (4.12.0.88)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 9)) (1.6.1)\n",
            "Collecting kornia>=0.7.2 (from -r requirements.txt (line 10))\n",
            "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0->-r requirements.txt (line 1)) (75.2.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch==2.4.0->-r requirements.txt (line 1))\n",
            "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.8->-r requirements.txt (line 3)) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.8->-r requirements.txt (line 3)) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.8->-r requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 7)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 7)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->-r requirements.txt (line 7)) (2025.10.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->-r requirements.txt (line 8)) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.6.0)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia>=0.7.2->-r requirements.txt (line 10))\n",
            "  Downloading kornia_rs-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 8)) (1.17.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm>=1.0.8->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0->-r requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Downloading torch-2.4.0-cp312-cp312-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.19.0-cp312-cp312-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.9-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, kornia_rs, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, kornia\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.23.0+cu126\n",
            "    Uninstalling torchvision-0.23.0+cu126:\n",
            "      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed kornia-0.8.1 kornia_rs-0.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchvision-0.19.0 triton-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c7dda67e",
      "metadata": {
        "id": "c7dda67e"
      },
      "outputs": [],
      "source": [
        "!unzip -qo \"archives/train_images.zip\" -d \"dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f4ba3a46",
      "metadata": {
        "id": "f4ba3a46"
      },
      "outputs": [],
      "source": [
        "!unzip -qo \"archives/test_images.zip\" -d \"dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "01efc7b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01efc7b8",
        "outputId": "71d0e7d3-0648-4919-aec0-9fc99123c027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataloaders...\n",
            "Loading LightGlue weights...\n",
            "Downloading: \"https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/superpoint_v1.pth\" to /root/.cache/torch/hub/checkpoints/superpoint_v1.pth\n",
            "100% 4.96M/4.96M [00:00<00:00, 355MB/s]\n",
            "Downloading: \"https://github.com/cvg/LightGlue/releases/download/v0.1_arxiv/superpoint_lightglue.pth\" to /root/.cache/torch/hub/checkpoints/superpoint_lightglue_v0-1_arxiv.pth\n",
            "100% 45.3M/45.3M [00:01<00:00, 43.3MB/s]\n",
            "Extracting keypoints for train set...\n",
            "Loading batches: 100% 171/171 [11:15<00:00,  3.95s/it]\n",
            "Extracting keypoints for test set...\n",
            "Loading batches: 100% 42/42 [02:38<00:00,  3.78s/it]\n",
            "✅ All keypoints detected and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "!python -m lightglue_keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "d-zP0taRuLg4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-zP0taRuLg4",
        "outputId": "ea5ae3a7-af0e-40a2-ad65-8bd3dfafb722"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 10926 files for split 'train'...\n",
            "train split: 100% 10926/10926 [02:35<00:00, 70.43it/s]\n",
            "Processing 2665 files for split 'test'...\n",
            "test split: 100% 2665/2665 [00:38<00:00, 69.87it/s]\n",
            "✅ All grids generated and saved successfully.\n"
          ]
        }
      ],
      "source": [
        "!python -m keypoints_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "d7df4f99",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7df4f99",
        "outputId": "48fe988d-5948-477b-e696-09ae06118368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive created: /content/CV_LUN_same_room_research/archives/train_keypoints.zip\n"
          ]
        }
      ],
      "source": [
        "!python -m scripts.archive_folder keypoints/train train_keypoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "qB_itVl95Z5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB_itVl95Z5d",
        "outputId": "60c40117-07ba-4734-861e-fb506edebc29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive created: /content/CV_LUN_same_room_research/archives/test_keypoints.zip\n"
          ]
        }
      ],
      "source": [
        "!python -m scripts.archive_folder keypoints/test test_keypoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15819e16",
      "metadata": {
        "id": "15819e16"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3dAj9f4X6GcO",
      "metadata": {
        "id": "3dAj9f4X6GcO"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, Subset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "uKo1zldo6N-m",
      "metadata": {
        "id": "uKo1zldo6N-m"
      },
      "outputs": [],
      "source": [
        "from path_config import (\n",
        "    ROOT,\n",
        "    DATASET_FOLDER_PATHS,\n",
        "    DATASET_ANNOTATIONS,\n",
        "    TRAIN_LABELS,\n",
        "    DETECTED_KEYPOINTS,\n",
        "    MODELS_PATH\n",
        ")\n",
        "\n",
        "LABEL_CSV = TRAIN_LABELS\n",
        "\n",
        "OUT_DIR = ROOT / \"runs\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TRAIN_DIR = DATASET_FOLDER_PATHS[\"train_folder_path\"]\n",
        "TEST_DIR  = DATASET_FOLDER_PATHS[\"test_folder_path\"]\n",
        "TRAIN_ANN = DATASET_ANNOTATIONS[\"train_annotation_path\"]\n",
        "TEST_ANN  = DATASET_ANNOTATIONS[\"test_annotation_path\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1fbbc509",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fbbc509",
        "outputId": "6c45a038-bddc-4cd9-8382-eda40b66e2c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c48631f4230>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 1337\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048d21d3",
      "metadata": {
        "id": "048d21d3"
      },
      "source": [
        "## DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "WYxXMd7m5j2C",
      "metadata": {
        "id": "WYxXMd7m5j2C"
      },
      "outputs": [],
      "source": [
        "class GridTrainDataset(Dataset):\n",
        "    def __init__(self, grid_dir: Path, label_csv: Path):\n",
        "        self.grid_dir = Path(grid_dir)\n",
        "        df = pd.read_csv(label_csv)\n",
        "\n",
        "        keep = []\n",
        "        for _, row in df.iterrows():\n",
        "            if (self.grid_dir / row[\"task_id\"] / \"grid_G32.npy\").exists():\n",
        "                keep.append(row)\n",
        "\n",
        "        self.df = pd.DataFrame(keep)\n",
        "        self.task_ids = self.df[\"task_id\"].values\n",
        "        self.labels = self.df[\"label\"].astype(np.float32).values\n",
        "\n",
        "    def __len__(self): return len(self.task_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tid = self.task_ids[idx]\n",
        "        y = self.labels[idx]\n",
        "        grid = np.load(self.grid_dir / tid / \"grid_G32.npy\")\n",
        "        grid = torch.tensor(grid, dtype=torch.float32)\n",
        "        return grid, torch.tensor(y, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "JB7tg-1n7O25",
      "metadata": {
        "id": "JB7tg-1n7O25"
      },
      "outputs": [],
      "source": [
        "class GridTestDataset(Dataset):\n",
        "    def __init__(self, grid_dir: Path):\n",
        "        self.grid_dir = Path(grid_dir)\n",
        "        self.task_ids = sorted([\n",
        "            p.name for p in self.grid_dir.iterdir()\n",
        "            if p.is_dir() and (p / \"grid_G32.npy\").exists()\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.task_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tid = self.task_ids[idx]\n",
        "        grid = np.load(self.grid_dir / tid / \"grid_G32.npy\")\n",
        "        grid = torch.tensor(grid, dtype=torch.float32)\n",
        "        return grid, tid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "SQqpC8n16Img",
      "metadata": {
        "id": "SQqpC8n16Img"
      },
      "outputs": [],
      "source": [
        "train_full = GridTrainDataset(DETECTED_KEYPOINTS[\"train\"], TRAIN_LABELS)\n",
        "\n",
        "idx_all = np.arange(len(train_full))\n",
        "y_all = train_full.labels.astype(int)\n",
        "\n",
        "def stratified_801010(idx_all, y_all, seed=SEED):\n",
        "    # need at least 2 samples of each class in each split for sklearn to behave\n",
        "    cls, counts = np.unique(y_all, return_counts=True)\n",
        "    do_strat = (len(cls) == 2) and all(counts >= 4)\n",
        "\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        idx_all, test_size=0.2, random_state=seed,\n",
        "        stratify=y_all if do_strat else None\n",
        "    )\n",
        "    val1_idx, val2_idx = train_test_split(\n",
        "        temp_idx, test_size=0.5, random_state=seed,\n",
        "        stratify=y_all[temp_idx] if do_strat else None\n",
        "    )\n",
        "    return train_idx, val1_idx, val2_idx\n",
        "\n",
        "train_idx, val1_idx, val2_idx = stratified_801010(idx_all, y_all, SEED)\n",
        "train_ds = Subset(train_full, train_idx.tolist())\n",
        "val1_ds  = Subset(train_full, val1_idx.tolist())\n",
        "val2_ds  = Subset(train_full, val2_idx.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "urPNIXvY9Jpo",
      "metadata": {
        "id": "urPNIXvY9Jpo"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=8, pin_memory=True)\n",
        "val1_loader  = DataLoader(val1_ds, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
        "val2_loader  = DataLoader(val2_ds, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d042e1b3",
      "metadata": {
        "id": "d042e1b3"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6fce8d55",
      "metadata": {
        "id": "6fce8d55"
      },
      "outputs": [],
      "source": [
        "# 1) Typed stems (7ch -> 3ch adapter), mask-gated\n",
        "class TypedStem(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.scalar = nn.Sequential(nn.Conv2d(3, 8, 1), nn.GroupNorm(1, 8), nn.GELU())\n",
        "\t\tself.motion = nn.Sequential(nn.Conv2d(2, 16, 3, padding=1), nn.GroupNorm(2, 16), nn.GELU())\n",
        "\t\tself.orient = nn.Sequential(nn.Conv2d(2, 8, 3, padding=1), nn.GroupNorm(1, 8), nn.GELU())\n",
        "\t\tself.drop = nn.Dropout2d(0.10)\n",
        "\t\tself.to3 = nn.Conv2d(8+16+8, 3, 1)\n",
        "\n",
        "\tdef forward(self, x):  # x: [B,7,32,32]\n",
        "\t\tm = (x[:, :1] > 0).float()\n",
        "\t\tsc = self.scalar(torch.cat([x[:,0:1], x[:,1:2], x[:,6:7]], dim=1))\n",
        "\t\tmn = self.motion(x[:,2:4])\n",
        "\t\tang = self.orient(x[:,4:6])\n",
        "\t\th = torch.cat([sc, mn, ang], dim=1) * m\n",
        "\t\th = self.drop(h)\n",
        "\t\treturn self.to3(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4139cbbe",
      "metadata": {
        "id": "4139cbbe"
      },
      "outputs": [],
      "source": [
        "# 2) Single-tower ConvNeXt feature encoder to 8x8 map\n",
        "class Tower(nn.Module):\n",
        "\tdef __init__(self, backbone='convnext_tiny', drop_path=0.10):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.stem = TypedStem()\n",
        "\t\tself.bk = timm.create_model(backbone, features_only=True, out_indices=(0,),\n",
        "\t\t                            pretrained=True, drop_path_rate=drop_path)\n",
        "\t\tfor p in self.bk.parameters(): p.requires_grad = False\n",
        "\n",
        "\tdef forward(self, x):  # x: [B,7,32,32]\n",
        "\t\tx = self.stem(x)  # -> [B,3,32,32]\n",
        "\t\tf = self.bk(x)[0] # -> [B,C,8,8]\n",
        "\t\tf = F.normalize(f, dim=1)  # L2 for cosine\n",
        "\t\treturn f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "a25a2216",
      "metadata": {
        "id": "a25a2216"
      },
      "outputs": [],
      "source": [
        "# 3) Local correlation volume (radius r)\n",
        "def local_corr(Fa, Fb, r=2):  # Fa,Fb: [B,C,8,8] -> [B,(2r+1)^2,8,8]\n",
        "\tB, C, H, W = Fa.shape\n",
        "\tpad = F.pad(Fb, (r, r, r, r))\n",
        "\tcosts = []\n",
        "\tfor oy in range(2*r+1):\n",
        "\t\tfor ox in range(2*r+1):\n",
        "\t\t\tsh = pad[:, :, oy:oy+H, ox:ox+W]\n",
        "\t\t\tcosts.append((Fa * sh).sum(1, keepdim=True))\n",
        "\treturn torch.cat(costs, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "e87c06bf",
      "metadata": {
        "id": "e87c06bf"
      },
      "outputs": [],
      "source": [
        "# 4) 3D cost-volume aggregator -> 2D map\n",
        "class Vol3D(nn.Module):\n",
        "\tdef __init__(self, c_out=16, p_drop=0.10):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.do3d = nn.Dropout3d(p_drop)\n",
        "\t\tself.net = nn.Sequential(\n",
        "\t\t\tnn.Conv3d(1, 8, 3, padding=1), nn.GELU(), nn.GroupNorm(1, 8),\n",
        "\t\t\tnn.Conv3d(8, 8, 3, padding=1), nn.GELU(),\n",
        "\t\t\tnn.Conv3d(8, c_out, 3, padding=1), nn.GELU()\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, vol):  # vol: [B,D,H,W]\n",
        "\t\tv = vol.unsqueeze(1)              # [B,1,D,H,W]\n",
        "\t\tv = self.do3d(v)                  # dropout along D,H,W\n",
        "\t\tv = self.net(v)                   # [B,Cd,D,H,W]\n",
        "\t\treturn v.mean(2)                  # -> [B,Cd,H,W]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "f8bd6c9a",
      "metadata": {
        "id": "f8bd6c9a"
      },
      "outputs": [],
      "source": [
        "# 5) Fusion + head\n",
        "class FusionHead(nn.Module):\n",
        "\tdef __init__(self, c_in_bk, c_vol=16, width=64, p_drop=0.20):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.pa = nn.Conv2d(c_in_bk, 16, 1)\n",
        "\t\tself.pb = nn.Conv2d(c_in_bk, 16, 1)\n",
        "\t\tself.pd = nn.Conv2d(c_in_bk, 16, 1)\n",
        "\t\tself.mix = nn.Conv2d(c_vol + 48, width, 1)\n",
        "\t\tself.drop = nn.Dropout(p_drop)\n",
        "\t\tself.mlp = nn.Sequential(nn.Linear(width, width//2), nn.GELU(), nn.Linear(width//2, 1))\n",
        "\n",
        "\tdef forward(self, agg2d, Fa, Fb):\n",
        "\t\tf = torch.cat([agg2d, self.pa(Fa), self.pb(Fb), self.pd(Fa - Fb)], dim=1)\n",
        "\t\tz = self.mix(f).mean(dim=(2,3))\n",
        "\t\tz = self.drop(z)\n",
        "\t\treturn self.mlp(z).squeeze(1)  # [B]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "8a29779c",
      "metadata": {
        "id": "8a29779c"
      },
      "outputs": [],
      "source": [
        "# 6) Full pair model\n",
        "class PairSameRoomModel(nn.Module):\n",
        "\tdef __init__(self, backbone='convnext_tiny', r=2):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.towerA = Tower(backbone)\n",
        "\t\tself.towerB = Tower(backbone)\n",
        "\t\t# peek a dummy to get backbone out-channels\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tc_bk = timm.create_model(backbone, features_only=True, out_indices=(0,), pretrained=False)(torch.zeros(1,3,32,32))[0].shape[1]\n",
        "\t\tself.r = r\n",
        "\t\tself.vol = Vol3D(c_out=16)\n",
        "\t\tself.head = FusionHead(c_in_bk=c_bk, c_vol=16, width=64)\n",
        "\n",
        "\tdef freeze_backbone(self, freeze=True):\n",
        "\t\tfor t in [self.towerA.bk, self.towerB.bk]:\n",
        "\t\t\tfor p in t.parameters(): p.requires_grad = not (freeze)\n",
        "\n",
        "\tdef forward(self, A7, B7):  # [B,7,32,32] each\n",
        "\t\tFa = self.towerA(A7)\n",
        "\t\tFb = self.towerB(B7)\n",
        "\t\tvol = local_corr(Fa, Fb, r=self.r)  # [B,D,8,8]\n",
        "\t\tagg = self.vol(vol)                 # [B,16,8,8]\n",
        "\t\treturn self.head(agg, Fa, Fb)       # logits [B]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-wRbxLW3EG6"
      },
      "source": [
        "## TRAINING"
      ],
      "id": "d-wRbxLW3EG6"
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "5af22a9c",
      "metadata": {
        "id": "5af22a9c"
      },
      "outputs": [],
      "source": [
        "model = PairSameRoomModel(backbone='convnext_tiny', r=2).to(DEVICE)\n",
        "model.freeze_backbone(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "fb4deff9",
      "metadata": {
        "id": "fb4deff9"
      },
      "outputs": [],
      "source": [
        "ckpt_path = MODELS_PATH / \"best_convnext_siamese.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "b25197d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25197d9",
        "outputId": "dfa5e874-c3b2-4542-d804-99dd1aa80dcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[train] pos=4672, neg=4068, pos_weight=0.871\n"
          ]
        }
      ],
      "source": [
        "y_train = y_all[train_idx]\n",
        "n_pos = int((y_train == 1).sum())\n",
        "n_neg = int((y_train == 0).sum())\n",
        "pos_weight_value = n_neg / max(n_pos, 1)\n",
        "\n",
        "print(f\"[train] pos={n_pos}, neg={n_neg}, pos_weight={pos_weight_value:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crit  = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value], device=DEVICE))\n",
        "scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJW5oaYI_hUt",
        "outputId": "a9b1c959-c039-4cad-d840-f713813c0ae9"
      },
      "id": "mJW5oaYI_hUt",
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2158985317.py:2: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(DEVICE.type == 'cuda'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WARMUP = False"
      ],
      "metadata": {
        "id": "71NdFEorBW2x"
      },
      "id": "71NdFEorBW2x",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "5c9fbce4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c9fbce4",
        "outputId": "bb5a2f07-4e07-49dd-e233-8f738293224f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[MODE] Fine-tune: unfrozen backbone, higher LR, cosine scheduler reset\n"
          ]
        }
      ],
      "source": [
        "if WARMUP:\n",
        "\tprint(\"[MODE] Warm-up: frozen backbone, small LR, cosine scheduler\")\n",
        "\tmodel.freeze_backbone(True)\n",
        "\tlr, wd = 3e-4, 1e-2\n",
        "\topt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.999))\n",
        "\tsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=5, eta_min=1e-6)\n",
        "else:\n",
        "\tprint(\"[MODE] Fine-tune: unfrozen backbone, higher LR, cosine scheduler reset\")\n",
        "\tmodel.freeze_backbone(False)\n",
        "\tlr, wd = 1e-4, 1e-2\n",
        "\topt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=(0.9, 0.999))\n",
        "\tsched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10, eta_min=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_grid(grid):\n",
        "\t# grid: [B,14,32,32] -> A7,B7\n",
        "\treturn grid[:, :7, ...], grid[:, 7:14, ...]\n",
        "\n",
        "def prep_batch_train(batch, device):\n",
        "\t# accepts (grid,y) or (A7,B7,y)\n",
        "\tif len(batch) == 3:\n",
        "\t\tA7, B7, y = batch\n",
        "\telse:\n",
        "\t\tgrid, y = batch\n",
        "\t\tA7, B7 = split_grid(grid)\n",
        "\treturn A7.to(device), B7.to(device), y.float().to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def prep_batch_eval(batch, device):\n",
        "\t# accepts (grid,y) only (for val loaders)\n",
        "\tif len(batch) == 3:\n",
        "\t\tA7, B7, y = batch\n",
        "\telse:\n",
        "\t\tgrid, y = batch\n",
        "\t\tA7, B7 = split_grid(grid)\n",
        "\treturn A7.to(device), B7.to(device), y.float().to(device)"
      ],
      "metadata": {
        "id": "VVD80wKS6o-Z"
      },
      "id": "VVD80wKS6o-Z",
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "af92792b",
      "metadata": {
        "id": "af92792b"
      },
      "outputs": [],
      "source": [
        "def train_step(batch, clip=1.0):\n",
        "\tmodel.train()\n",
        "\tA7, B7, y = prep_batch_train(batch, DEVICE)\n",
        "\n",
        "\topt.zero_grad(set_to_none=True)\n",
        "\twith autocast(enabled=scaler is not None):\n",
        "\t\tlogits = model(A7, B7)\n",
        "\t\tloss = crit(logits, y)\n",
        "\n",
        "\tscaler.scale(loss).backward()\n",
        "\tscaler.unscale_(opt)\n",
        "\tnn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\tscaler.step(opt)\n",
        "\tscaler.update()\n",
        "\tsched.step()\n",
        "\treturn float(loss.detach()), opt.param_groups[0]['lr']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "65805b94",
      "metadata": {
        "id": "65805b94"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def predict_proba(model, loader, device=DEVICE):\n",
        "\tmodel.eval()\n",
        "\tps, ys = [], []\n",
        "\tfor batch in loader:\n",
        "\t\tA7, B7, y = prep_batch_eval(batch, device)\n",
        "\t\tp = torch.sigmoid(model(A7, B7)).cpu()\n",
        "\t\tps.append(p); ys.append(y.cpu())\n",
        "\treturn torch.cat(ps), torch.cat(ys)\n",
        "\n",
        "def f1_pr_rec_at_threshold(p, y, thr):\n",
        "\tif hasattr(p, \"detach\"): p = p.detach().cpu().numpy()\n",
        "\tif hasattr(y, \"detach\"): y = y.detach().cpu().numpy().astype(int)\n",
        "\tpb = (p >= thr).astype(int)\n",
        "\treturn (\n",
        "\t\tfloat(f1_score(y, pb, zero_division=0)),\n",
        "\t\tfloat(precision_score(y, pb, zero_division=0)),\n",
        "\t\tfloat(recall_score(y, pb, zero_division=0)),\n",
        "\t)\n",
        "\n",
        "def pick_best_threshold(p, y):\n",
        "\tif hasattr(p, \"detach\"): p = p.detach().cpu().numpy()\n",
        "\tif hasattr(y, \"detach\"): y = y.detach().cpu().numpy().astype(int)\n",
        "\n",
        "\tprec, rec, th = precision_recall_curve(y, p)\n",
        "\tf1 = 2*prec*rec/(prec+rec+1e-9)\n",
        "\tf1_t = f1[1:]\n",
        "\ti = int(np.nanargmax(f1_t))\n",
        "\treturn {\"thr\": float(th[i]), \"f1\": float(f1_t[i]),\n",
        "\t        \"prec\": float(prec[i+1]), \"rec\": float(rec[i+1])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "488ea8d8",
      "metadata": {
        "id": "488ea8d8"
      },
      "outputs": [],
      "source": [
        "def save_ckpt(path, model, opt, epoch, best_thr, best_val1_f1, backbone='convnext_tiny', r=2):\n",
        "    torch.save({\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"optimizer_state\": opt.state_dict(),\n",
        "        \"epoch\": epoch,\n",
        "        \"best_thr\": float(best_thr),\n",
        "        \"best_val1_f1\": float(best_val1_f1),\n",
        "        \"config\": {\"backbone\": backbone, \"r\": r}\n",
        "    }, path)\n",
        "\n",
        "def load_ckpt(path, model, opt=None, map_location=\"cpu\"):\n",
        "\tdata = torch.load(path, map_location=map_location)\n",
        "\tmodel.load_state_dict(data[\"model_state\"])\n",
        "\tif opt is not None and \"optimizer_state\" in data:\n",
        "\t\topt.load_state_dict(data[\"optimizer_state\"])\n",
        "\treturn data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "142aa268",
      "metadata": {
        "id": "142aa268"
      },
      "outputs": [],
      "source": [
        "best = {\"f1\": 0.0, \"thr\": 0.5, \"epoch\": 0}\n",
        "epochs = 15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "72bf3673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72bf3673",
        "outputId": "2552f91f-719f-4070-d3a6-1e9410c05eab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | loss 0.3904 | lr 3.52e-05 | val1 F1 0.8388 @ 0.38 (P 0.849, R 0.829)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 002 | loss 0.3668 | lr 1.05e-05 | val1 F1 0.8466 @ 0.47 (P 0.889, R 0.808)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 003 | loss 0.3486 | lr 9.05e-05 | val1 F1 0.8348 @ 0.47 (P 0.867, R 0.805)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 004 | loss 0.3372 | lr 6.58e-05 | val1 F1 0.8463 @ 0.37 (P 0.874, R 0.820)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 005 | loss 0.3157 | lr 1.00e-06 | val1 F1 0.8382 @ 0.28 (P 0.843, R 0.834)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 006 | loss 0.2892 | lr 6.58e-05 | val1 F1 0.8304 @ 0.16 (P 0.811, R 0.851)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 007 | loss 0.2460 | lr 9.05e-05 | val1 F1 0.8310 @ 0.42 (P 0.801, R 0.863)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 008 | loss 0.2267 | lr 1.05e-05 | val1 F1 0.8382 @ 0.46 (P 0.821, R 0.856)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 009 | loss 0.1862 | lr 3.52e-05 | val1 F1 0.8395 @ 0.27 (P 0.820, R 0.860)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010 | loss 0.1647 | lr 1.00e-04 | val1 F1 0.8340 @ 0.65 (P 0.805, R 0.865)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 011 | loss 0.1303 | lr 3.52e-05 | val1 F1 0.8231 @ 0.08 (P 0.785, R 0.865)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 012 | loss 0.1076 | lr 1.05e-05 | val1 F1 0.8095 @ 0.50 (P 0.865, R 0.760)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 013 | loss 0.0897 | lr 9.05e-05 | val1 F1 0.8131 @ 0.26 (P 0.869, R 0.764)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 014 | loss 0.1102 | lr 6.58e-05 | val1 F1 0.8232 @ 0.54 (P 0.860, R 0.789)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1809081031.py:6: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=scaler is not None):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 015 | loss 0.0646 | lr 1.00e-06 | val1 F1 0.8214 @ 0.09 (P 0.791, R 0.854)\n",
            "[TRAIN DONE] best val1 F1=0.8470 at epoch=13 with thr=0.36. Checkpoint: /content/CV_LUN_same_room_research/models/best_convnext_siamese.pt\n"
          ]
        }
      ],
      "source": [
        "for ep in range(1, epochs + 1):\n",
        "\t# ---- train ----\n",
        "\tmodel.train()\n",
        "\trunning = 0.0\n",
        "\tfor batch in train_loader:\n",
        "\t\tloss, lr = train_step(batch)\n",
        "\t\trunning += loss\n",
        "\tavg_loss = running / max(1, len(train_loader))\n",
        "\n",
        "\t# ---- val1: pick threshold & metrics ----\n",
        "\tp1, y1 = predict_proba(model, val1_loader, device=DEVICE)\n",
        "\tsel = pick_best_threshold(p1, y1)  # {'thr','f1','prec','rec'}\n",
        "\tval1_thr = sel[\"thr\"]\n",
        "\tval1_f1, val1_pr, val1_rc = sel[\"f1\"], sel[\"prec\"], sel[\"rec\"]\n",
        "\n",
        "\t# ---- log ----\n",
        "\tprint(f\"Epoch {ep:03d} | loss {avg_loss:.4f} | lr {lr:.2e} | \"\n",
        "\t      f\"val1 F1 {val1_f1:.4f} @ {val1_thr:.2f} (P {val1_pr:.3f}, R {val1_rc:.3f})\")\n",
        "\n",
        "\t# ---- checkpoint on val1 improvement ----\n",
        "\tif val1_f1 > best[\"f1\"] + 1e-6:\n",
        "\t\tbest.update({\"f1\": val1_f1, \"thr\": val1_thr, \"epoch\": ep})\n",
        "\t\tsave_ckpt(ckpt_path, model, opt, ep, best[\"thr\"], best[\"f1\"])\n",
        "\n",
        "print(f\"[TRAIN DONE] best val1 F1={best['f1']:.4f} at epoch={best['epoch']} with thr={best['thr']:.2f}. \"\n",
        "      f\"Checkpoint: {os.path.abspath(ckpt_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_ckpt(ckpt_path, model, opt=None, map_location=DEVICE)\n",
        "thr = float(data.get(\"best_thr\"))\n",
        "\n",
        "p2, y2 = predict_proba(model, val2_loader, device=DEVICE)\n",
        "y2_bin = (p2.numpy() >= thr).astype(int)\n",
        "\n",
        "print(f\"val2 @ thr={thr:.4f}\")\n",
        "print(classification_report(y2.numpy().astype(int), y2_bin, digits=4))\n",
        "print(\"Confusion matrix:\\n\", confusion_matrix(y2.numpy().astype(int), y2_bin))\n",
        "\n",
        "print(f\"ROC-AUC: {roc_auc_score(y2, p2):.4f}\")\n",
        "print(f\"PR-AUC : {average_precision_score(y2, p2):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOoZtXYIzcpv",
        "outputId": "6f97f101-1f2b-4e74-bd61-a62b4ca75a14"
      },
      "id": "KOoZtXYIzcpv",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3071334797.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data = torch.load(path, map_location=map_location)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val2 @ thr=0.3561\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7644    0.8350    0.7981       509\n",
            "           1     0.8436    0.7757    0.8082       584\n",
            "\n",
            "    accuracy                         0.8033      1093\n",
            "   macro avg     0.8040    0.8053    0.8032      1093\n",
            "weighted avg     0.8067    0.8033    0.8035      1093\n",
            "\n",
            "Confusion matrix:\n",
            " [[425  84]\n",
            " [131 453]]\n",
            "ROC-AUC: 0.8743\n",
            "PR-AUC : 0.9132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c07f18aa",
      "metadata": {
        "id": "c07f18aa"
      },
      "source": [
        "## TESTING"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = GridTestDataset(DETECTED_KEYPOINTS[\"test\"])\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "6rNbPSJ83KA7"
      },
      "id": "6rNbPSJ83KA7",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "\tfor grids, tids in test_loader:\n",
        "\t\tA7, B7 = grids[:, :7, ...].to(DEVICE), grids[:, 7:, ...].to(DEVICE)\n",
        "\t\tp = torch.sigmoid(model(A7, B7)).cpu().numpy().ravel()\n",
        "\t\tlabels = (p >= thr).astype(int)\n",
        "\t\tpreds.extend(zip(tids, labels))\n",
        "\n",
        "df_pred = pd.DataFrame(preds, columns=[\"task_id\", \"label\"])\n",
        "df_pred.to_csv(\"predictions.csv\", index=False)\n",
        "print(df_pred.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCbKBY8sDvjt",
        "outputId": "0ce26fec-b1da-41bd-a3c8-ec223db3e334"
      },
      "id": "RCbKBY8sDvjt",
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               task_id  label\n",
            "0  427095798#427095776      1\n",
            "1  775290228#777300509      0\n",
            "2  777223067#777223065      1\n",
            "3  777235160#777235113      1\n",
            "4  777250505#777250510      0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVING"
      ],
      "metadata": {
        "id": "GVZLbpdMIy1G"
      },
      "id": "GVZLbpdMIy1G"
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_path = MODELS_PATH / \"best_convnext_siamese.pt\"\n",
        "ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "thr = float(ckpt.get(\"best_thr\", 0.5))\n",
        "backbone = ckpt[\"config\"].get(\"backbone\", \"convnext_tiny\")\n",
        "r = int(ckpt[\"config\"].get(\"r\", 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNM_IsmCI12V",
        "outputId": "7b59dbce-57ab-43e1-81ea-497401b8db59"
      },
      "id": "yNM_IsmCI12V",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-735217627.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = PairSameRoomModel(backbone=backbone, r=r).eval()\n",
        "m.load_state_dict(ckpt[\"model_state\"])\n",
        "class BinaryWrap(nn.Module):\n",
        "    def __init__(self, base, thr):\n",
        "        super().__init__()\n",
        "        self.base = base.eval()\n",
        "        self.register_buffer(\"thr\", torch.tensor(thr, dtype=torch.float32))\n",
        "    def forward(self, A7: torch.Tensor, B7: torch.Tensor) -> torch.Tensor:\n",
        "        return (torch.sigmoid(self.base(A7, B7)) >= self.thr).to(torch.int64)\n",
        "\n",
        "wrap = BinaryWrap(m, thr).eval()"
      ],
      "metadata": {
        "id": "ECpKAjobKyZ6"
      },
      "id": "ECpKAjobKyZ6",
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exA = torch.zeros(1,7,32,32); exB = torch.zeros(1,7,32,32)\n",
        "ts = torch.jit.trace(wrap, (exA, exB))\n",
        "ts_path = MODELS_PATH / \"same_room_binary.ts\"\n",
        "ts.save(ts_path)\n",
        "print(\"Saved:\", os.path.abspath(ts_path), \"thr=\", thr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viXcz0DdK0Oo",
        "outputId": "1a430c22-d4be-4a2a-f12f-2503f75aa559"
      },
      "id": "viXcz0DdK0Oo",
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/CV_LUN_same_room_research/models/same_room_binary.ts thr= 0.3560520112514496\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "40293603",
        "15819e16",
        "048d21d3",
        "d042e1b3"
      ]
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}